{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2 import apply_rotary_emb,precompute_freqs_cis,Attention\n",
    "import torch\n",
    "\n",
    "xq = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "xk = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "\n",
    "# 使用precomputed_freqs_cis 获取sin和cos\n",
    "cos,sin = precompute_freqs_cis(288//6, 50)\n",
    "print(cos.shape,sin.shape) \n",
    "xq_out,xk_out = apply_rotary_emb(xq, xk, cos, sin)\n",
    "xq_out.shape,xk_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape torch.Size([1, 50, 768])\n",
      "freqs_cos shape torch.Size([50, 24]), freqs_sin shape torch.Size([50, 24])\n",
      "调整维度之前： xq shape torch.Size([1, 50, 768]), xk shape torch.Size([1, 50, 384]), xv shape torch.Size([1, 50, 384]) \n",
      "调整维度之后： xq shape torch.Size([1, 50, 16, 48]), xk shape torch.Size([1, 50, 8, 48]), xv shape torch.Size([1, 50, 8, 48]) \n",
      "旋转位置嵌入： xq shape torch.Size([1, 50, 16, 48]), xk shape torch.Size([1, 50, 8, 48])\n",
      "批次维度处理： xq shape torch.Size([1, 16, 50, 48]), xk shape torch.Size([1, 16, 50, 48]), xv shape torch.Size([1, 16, 50, 48]) \n",
      "Output shape: torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "from llama2 import Attention,ModelConfig,precompute_freqs_cis\n",
    "import torch\n",
    "\n",
    "args = ModelConfig()\n",
    "# 创建Attention实例\n",
    "attention_model = Attention(args)\n",
    "\n",
    "# 模拟输入数据\n",
    "batch_size = 1\n",
    "seq_len = 50  # 假设实际使用的序列长度为50\n",
    "dim = args.dim\n",
    "x = torch.rand(batch_size, seq_len, dim)  # 随机生成输入张量\n",
    "print(f\"x shape {x.shape}\")\n",
    "# freqs_cos = torch.rand(seq_len, dim // 2)  # 模拟cos频率，用于RoPE\n",
    "# freqs_sin = torch.rand(seq_len, dim // 2)  # 模拟sin频率，用于RoPE\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "print(f\"freqs_cos shape {freqs_cos.shape}, freqs_sin shape {freqs_sin.shape}\")\n",
    "\n",
    "\n",
    "# 运行Attention模型\n",
    "output = attention_model(x, freqs_cos, freqs_sin)\n",
    "\n",
    "# attention出来之后的形状 依然是[batch_size, seq_len, dim]\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36700ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:768 hidden_dim:2048 mutile_of:64 dropout:0.0\n",
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "from llama2 import Attention,ModelConfig,MLP\n",
    "import torch\n",
    "\n",
    "args = ModelConfig()\n",
    "# 创建MLP实例\n",
    "mlp = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)\n",
    "# 随机生成数据\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "# 运行MLP模型\n",
    "output = mlp(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515886ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim:768 hidden_dim:2048 mutile_of:64 dropout:0.0\n",
      "torch.Size([1, 50, 768])\n",
      "调整维度之前： xq shape torch.Size([1, 50, 768]), xk shape torch.Size([1, 50, 384]), xv shape torch.Size([1, 50, 384]) \n",
      "调整维度之后： xq shape torch.Size([1, 50, 16, 48]), xk shape torch.Size([1, 50, 8, 48]), xv shape torch.Size([1, 50, 8, 48]) \n",
      "旋转位置嵌入： xq shape torch.Size([1, 50, 16, 48]), xk shape torch.Size([1, 50, 8, 48])\n",
      "批次维度处理： xq shape torch.Size([1, 16, 50, 48]), xk shape torch.Size([1, 16, 50, 48]), xv shape torch.Size([1, 16, 50, 48]) \n",
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "from llama2 import Attention,ModelConfig,MLP,DecoderLayer,precompute_freqs_cis\n",
    "import torch\n",
    "\n",
    "args = ModelConfig()\n",
    "\n",
    "# 创建LLaMADecoderLayer实例\n",
    "decoderlayer = DecoderLayer(0, args)\n",
    "\n",
    "# 模拟输入数据\n",
    "dim = args.dim\n",
    "seq_len = 50\n",
    "\n",
    "x = torch.randn(1, seq_len, dim) # [bs, seq_len, dim]\n",
    "print(x.shape)\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "out = decoderlayer(x, freqs_cos, freqs_sin)\n",
    "\n",
    "print(out.shape) # 形状和输入的x一样 [batch_size, seq_len, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2407f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[1585, 2038, 4952, 3093, 5099, 2860, 2580, 2518,  634, 3128, 1349, 4460,\n",
      "         3617, 5235,  275, 1986, 5783,  463,  931, 2584, 2447, 4277,  820, 5148,\n",
      "         4094, 1558, 5582, 4577, 3232, 2681, 2609, 1664, 1732, 3448, 1200, 2254,\n",
      "         1621, 4233, 4794, 3279, 4542, 2308, 5400, 3364, 2047, 5510,  177,  679,\n",
      "         5949, 4153]])\n",
      "Number of parameters: 82594560\n",
      "out tensor([[[-0.1328,  0.2435, -0.3125,  ...,  0.5583, -0.6154,  0.6677]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([1, 1, 6144])\n"
     ]
    }
   ],
   "source": [
    "from llama2 import Attention,ModelConfig,MLP,DecoderLayer,precompute_freqs_cis,Transformer\n",
    "import torch\n",
    "args = ModelConfig()\n",
    "# LLaMA2Model.forward 接受两个参数，tokens和targets，其中tokens是输入的张量, 应为int类型\n",
    "x = torch.randint(0, 6144, (1, 50)) # [bs, seq_len]\n",
    "print (f\"x {x}\")\n",
    "# 实例化LLaMA2Model\n",
    "model = Transformer(args=args)\n",
    "# 计算model的全部参数\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters:', num_params)\n",
    "\n",
    "out = model(x)\n",
    "print(f\"out {out.logits}\")\n",
    "print(out.logits.shape) # [batch_size, 1, vocab_size]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
